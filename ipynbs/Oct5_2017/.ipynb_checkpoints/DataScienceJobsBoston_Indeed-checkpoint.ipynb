{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://www.rocketsoftware.com/sites/all/themes/rocketon/logo.png\" align='top' height=200 width=400>\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "\n",
    "<span style=\"color:#3357A6; font-family: 'Comic Sans MS';font-size: 14pt\">Charles Aydin<br>\n",
    "Data Scientist<br>\n",
    "caydin@rocketsoftware.com<br>\n",
    "+1 781 577 4445 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<html>\n",
    "<body>\n",
    "\n",
    "<h2>Objective</h2>\n",
    "<br><br>\n",
    "<span style=\"color:#3357A6; font-family: 'Comic Sans MS';font-size: 12pt\">\n",
    "<ul style=\"list-style-type:disc\">\n",
    "    <li>Collect data by scraping Indeed.com for \"Data Scientist\" (and similar) queries</li><br>\n",
    "    <li>Build a binary (High/Low) classifier for predicting salaries.</li><br>\n",
    "            <ul>\n",
    "              <li>Predicting a range be may be more useful due to \"natural\" variance in job salaries</li>\n",
    "            </ul>\n",
    "</ul>\n",
    "</span>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Indeed.com is a simple text page and relevant entries can easily be found via scraping.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=0B98hcul3bVqFVnFqOGEyUW5KZE0\" align='top' height=600 width=1200>\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import urllib\n",
    "#import urllib2\n",
    "#import lxml\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# defining my data extractor, this takes the page html from beautifulsoup \n",
    "# as the input and returns pandas df with the job name, location, salary \n",
    "# and description columns\n",
    "\n",
    "def data_extractor(crawler):\n",
    "    idx = [\"title\", \"co\", \"location\", \"sal\", \"des\"]\n",
    "    tempdf = pd.DataFrame(columns = idx) #temp DF to return with all job data\n",
    "    \n",
    "    jobs = crawler.find_all('div', attrs={'data-tn-component': 'organicJob'})\n",
    "    for job in jobs:\n",
    "        title= job.h2.a['title']\n",
    "        company = job.span.span.text.strip()\n",
    "        loc = job.findAll(\"span\", {\"itemprop\": \"addressLocality\"})\n",
    "        location = loc[0].text\n",
    "        no_wrap = job.find('td', {'class':'snip'}).find(\"span\", {\"class\":\"no-wrap\"})\n",
    "        if no_wrap == None:\n",
    "            salary = None\n",
    "        else:\n",
    "            salary= no_wrap.text.strip()\n",
    "        \n",
    "        description = job.find(\"span\", {\"itemprop\":\"description\"})\n",
    "        description = description.text.strip()\n",
    "\n",
    "        newrow=pd.Series([title, company, location, salary, description], index = idx)\n",
    "        tempdf = tempdf.append(newrow, ignore_index = 1)\n",
    "        \n",
    "    return tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First try for Boston\n",
    "\n",
    "base = \"http://www.indeed.com\"\n",
    "idx = [\"title\", \"co\", \"loc\", \"sal\", \"des\"]\n",
    "bos_jobs = pd.DataFrame(columns = idx) \n",
    "\n",
    "query_list =  [\"data scientist\"] \n",
    "\n",
    "city_list = [\"boston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib' has no attribute 'pathname2url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d6b3ed516e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://www.indeed.com/jobs?jt=fulltime&limit=100&q=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathname2url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m        \u001b[0;34m\"&l=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathname2url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#set up first page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mnew_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'pathname2url'"
     ]
    }
   ],
   "source": [
    "for query in query_list:\n",
    "    for city in city_list:\n",
    "        start = datetime.now()\n",
    "\n",
    "        URL = \"http://www.indeed.com/jobs?jt=fulltime&limit=100&q=\"+urllib.pathname2url(query)+\\\n",
    "        \"&l=\" + urllib.pathname2url(city)#set up first page\n",
    "\n",
    "        new_page = urllib2.urlopen(URL).read()\n",
    "        crawler = BS(new_page, 'html.parser') #structure the page with the parser\n",
    "\n",
    "        bos_jobs = bos_jobs.append(data_extractor(crawler)) #extract first page using function\n",
    "        \n",
    "        # Now the while loop will search for the 'next' button and take the href from there and \n",
    "        # update link and will call the data_extractor on it which will continuously append \n",
    "        # to alljobs DF\n",
    "        \n",
    "        # need to wrap the while loop in a \"try\" in case there is only one page and there is no\n",
    "        # \"next\"\n",
    "        try:\n",
    "            while crawler.findAll('span',{'class':'np'})[-1].text[:4] == 'Next':\n",
    "\n",
    "                URL = crawler.findAll('span',{'class':'np'})[-1].findParent('a')['href']\n",
    "                new_page = urllib2.urlopen(base + URL).read()\n",
    "\n",
    "                time.sleep(3) # little nap between downloads so that I dont get GA ip \n",
    "                                # adress banned\n",
    "                crawler = BS(new_page, 'html.parser')\n",
    "\n",
    "                # individual page parser:\n",
    "\n",
    "                bos_jobs = bos_jobs.append(data_extractor(crawler))\n",
    "            else: \n",
    "                print(\"finished with \", query , \"in \", city) #just a ticker to let me know it \n",
    "                                                            # is cycling through\n",
    "        \n",
    "        except IndexError:\n",
    "            bos_jobs = bos_jobs.append(data_extractor(crawler))\n",
    "            print(\"finished with \", query , \"in \", city)\n",
    "            \n",
    "        end = datetime.now()\n",
    "        diff = end - start\n",
    "        print('Total performance test time: {}'.format(diff.total_seconds()))\n",
    "            \n",
    "\n",
    "print(\"ALL DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jobswithsal_Boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-84e6967e209f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjobswithsal_Boston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'jobswithsal_Boston' is not defined"
     ]
    }
   ],
   "source": [
    "jobswithsal_Boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First try for Boston\n",
    "\n",
    "base = \"http://www.indeed.com\"\n",
    "idx = [\"title\", \"co\", \"loc\", \"sal\", \"des\"]\n",
    "bos_jobs = pd.DataFrame(columns = idx) \n",
    "\n",
    "query_list =  [\"data scientist\"] \n",
    "\n",
    "city_list = [\"boston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for query in query_list:\n",
    "    for city in city_list:\n",
    "        start = datetime.now()\n",
    "\n",
    "        URL = \"http://www.indeed.com/jobs?jt=fulltime&limit=100&q=\"+urllib.pathname2url(query)+\\\n",
    "        \"&l=\" + urllib.pathname2url(city)#set up first page\n",
    "\n",
    "        new_page = urllib2.urlopen(URL).read()\n",
    "        crawler = BS(new_page, 'html.parser') #structure the page with the parser\n",
    "\n",
    "        bos_jobs = bos_jobs.append(data_extractor(crawler)) #extract first page using function\n",
    "        \n",
    "        # Now the while loop will search for the 'next' button and take the href from there and \n",
    "        # update link and will call the data_extractor on it which will continuously append \n",
    "        # to alljobs DF\n",
    "        \n",
    "        # need to wrap the while loop in a \"try\" in case there is only one page and there is no\n",
    "        # \"next\"\n",
    "        try:\n",
    "            while crawler.findAll('span',{'class':'np'})[-1].text[:4] == 'Next':\n",
    "\n",
    "                URL = crawler.findAll('span',{'class':'np'})[-1].findParent('a')['href']\n",
    "                new_page = urllib2.urlopen(base + URL).read()\n",
    "\n",
    "                time.sleep(.25) # little nap between downloads so that I dont get GA ip \n",
    "                                # adress banned\n",
    "                crawler = BS(new_page, 'html.parser')\n",
    "\n",
    "                # individual page parser:\n",
    "\n",
    "                bos_jobs = bos_jobs.append(data_extractor(crawler))\n",
    "            else: \n",
    "                print(\"finished with \", query , \"in \", city) #just a ticker to let me know it \n",
    "                                                            # is cycling through\n",
    "        \n",
    "        except IndexError:\n",
    "            bos_jobs = bos_jobs.append(data_extractor(crawler))\n",
    "            print(\"finished with \", query , \"in \", city)\n",
    "            \n",
    "        end = datetime.now()\n",
    "        diff = end - start\n",
    "        print('Total performance test time: {}'.format(diff.total_seconds()))\n",
    "            \n",
    "\n",
    "print(\"ALL DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bos_jobs_wSal = bos_jobs[-bos_jobs.sal.isnull()]\n",
    "bos_jobs_wSal.sal = [_.replace(\",\",\"\").replace(\"$\",\"\") \n",
    "                                      for _ in bos_jobs_wSal.sal ] \n",
    "#get the commas and $ out of there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bos_jobs_wSal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows with hourly salaries\n",
    "hourly_sal = bos_jobs_wSal.sal[(bos_jobs_wSal.sal.str.contains(\"hour\"))].index \n",
    "bos_jobs_wSal.drop(hourly_sal, inplace =True)\n",
    "bos_jobs_wSal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# detect the data instances with ranges of salary\n",
    "sal_range = bos_jobs_wSal.sal.str.contains(\"-\")\n",
    "\n",
    "# detect the data instances with the phrase 'a year'\n",
    "year_word = bos_jobs_wSal.sal.str.contains(\"a year\")\n",
    "\n",
    "# Remove the ' a year' phrase\n",
    "bos_jobs_wSal.sal = [ _.split(\"a year\",1) for _ in bos_jobs_wSal.sal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bos_jobs_wSal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#average out the range of salary columns and convert to float\n",
    "bos_jobs_wSal.sal[sal_range] =[0.5*(float(x[0].split()[0])+float(x[0].split()[2])) \\\n",
    "                               for x in bos_jobs_wSal.sal[sal_range]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the remaining (\"non-range\") salaries to float\n",
    "bos_jobs_wSal.sal[-sal_range] =[float(x[0]) for x in bos_jobs_wSal.sal[-sal_range]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Check the size of the data\n",
    "len(bos_jobs_wSal.sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change \"loc\" to \"location\"\n",
    "bos_jobs_wSal.columns= [\"title\", \"co\", \"location\", \"sal\", \"des\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bos_jobs_wSal[\"cities\"] = [_.split(\",\")[0].replace(\",\",\"\") for _ in bos_jobs_wSal.location]\n",
    "\n",
    "states=[]\n",
    "for location in bos_jobs_wSal.location:\n",
    "    try:\n",
    "        states.append(location.split(\",\")[1].split()[0].replace(\",\",\"\"))\n",
    "    except IndexError:\n",
    "        states.append(location.split()[0].replace(\",\",\"\"))\n",
    "\n",
    "bos_jobs_wSal[\"states\"] = states\n",
    "\n",
    "#adding in states and cities as a datapoint    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "descriptions = [_.lower().replace(\",\",\"\").replace(\".\",\"\").replace(\"...\",\"\").replace(\"!\",\"\").replace(\"/\", \" \")\n",
    "                for _ in bos_jobs_wSal.des]\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "for description in descriptions:\n",
    "    for word in description.split():\n",
    "        if word in wordcount.keys():\n",
    "            wordcount[word] +=1\n",
    "        else:\n",
    "            wordcount[word] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[desc for desc in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
